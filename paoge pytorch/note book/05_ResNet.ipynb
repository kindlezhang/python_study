{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "61565fa0",
   "metadata": {},
   "source": [
    "# ResNet\n",
    "\n",
    "## Basic knowledge\n",
    "\n",
    "在残差块之前，如果模型深度太深，可能会带来梯度消失或者梯度爆炸的问题\n",
    "正则化，dropout等就是为了缓解这个问题\n",
    "但是随着layer深度加深，又会出现模型退化的问题\n",
    "\n",
    "因为训练过程中，现行修正单元relu处理后难免会有特征的信息缺失，就算是让w保持不变也很难做到，所以简单的读碟layer必然会带来退化问题\n",
    "\n",
    "ResNet其实就是在迭代的过程中， a = x + h(x), 这样可以保证a中一定保留了一些有用信息， 这就是一个residual block\n",
    "\n",
    "## residual block\n",
    "\n",
    "<img src=\"../graph/20.png\" width=\"500\" height=\"250\" />\n",
    "\n",
    "有两种，如果形状一样就不用额外卷积，直接相加就行，否则需要用一个卷积核调整大小\n",
    "\n",
    "for ex:\n",
    "\n",
    "<img src=\"../graph/21.png\" width=\"500\" height=\"250\" />\n",
    "\n",
    "归一化层BN就是把值归一化成0-1的数，大小不变\n",
    "\n",
    "特征图的大小经过一系列操作又变回了原来的大小，可以和自己相加，相加后shape不变，数值对应相加。（也可以不用和原先shape相同，只要保证AB路径能相加就行）\n",
    "\n",
    "<img src=\"../graph/22.png\" width=\"500\" height=\"250\" />\n",
    "\n",
    "填充应该是0，图有问题\n",
    "\n",
    "## batch Normalization\n",
    "\n",
    "2015由Google提出，一个深度神经训练的技巧，就是对每一批数据进行归一化，归一化操作从此不必只出现在输入层\n",
    "\n",
    "通过归一化操作，消除物理量纲，将所有特征放在同一起跑线，突出weight对最后结果的影响\n",
    "\n",
    "<img src=\"../graph/23.png\" width=\"500\" height=\"250\" />\n",
    "\n",
    "<img src=\"../graph/24.png\" width=\"500\" height=\"250\" />\n",
    "\n",
    "加快参数训练的速度\n",
    "\n",
    "从直观的角度来讲，归一化使特征值总是会被移到0中间，如图sigmoid的梯度曲线，这样可以使梯度不会爆炸也不会消失\n",
    "\n",
    "<img src=\"../graph/25.png\" width=\"500\" height=\"250\" />\n",
    "\n",
    "简单的变成正态分布会损失一些网络表达能力，所以线性变换一定程度上缓解这个问题，参数机器可以自己学习\n",
    "\n",
    "<img src=\"../graph/26.png\" width=\"500\" height=\"250\" />\n",
    "\n",
    "注意是梯度的变化不大，不是梯度不大\n",
    "\n",
    "总结BN的作用\n",
    "1. 加快模型训练\n",
    "2. 防止梯度消失和爆炸\n",
    "3. 不用谨慎地设计权重初始化，对于一个单元的输入值，不管是权重w，还是放缩后的kw，BN过后的值都是一样的了，k被消除掉了\n",
    "   \n",
    "## ResNet parameter\n",
    "\n",
    "Ex: ResNet18:\n",
    "\n",
    "<img src=\"../graph/27.png\" width=\"500\" height=\"250\" />\n",
    "\n",
    "再经过两个相同的不带B路径调整（1*1卷积核）的残差块\n",
    "\n",
    "<img src=\"../graph/28.png\" width=\"500\" height=\"250\" />\n",
    "\n",
    "<img src=\"../graph/29.png\" width=\"500\" height=\"250\" />\n",
    "\n",
    "<img src=\"../graph/30.png\" width=\"500\" height=\"250\" />\n",
    "\n",
    "第6，7，8同理\n",
    "\n",
    "7*7*512进入全局平均池化， 得到1*1*512\n",
    "\n",
    "Flatten层：\n",
    "输入为1*1*512， 输出为1*512\n",
    "\n",
    "DNN\n",
    "输入1*512，神经元个数为10， 最后用softmax转为概率，和为1， 最大值为分类结果。\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
