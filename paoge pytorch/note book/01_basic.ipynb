{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "beb684c0",
   "metadata": {},
   "source": [
    "# basic knowledge\n",
    "\n",
    "## DNN结构\n",
    "\n",
    "输入层 隐藏层 输出层\n",
    "输入层是一些图片或者矩阵\n",
    "同层的神经单元之间相互独立，不同层之间的神经元全连接\n",
    "\n",
    "## DNN单元\n",
    "\n",
    "神经元的数学表达式：\n",
    "a = h(w * x + b)\n",
    "b是bias, w是权重矩阵，x是输入矩阵，a是下一个神经元，h是<mark>激活函数</mark>\n",
    "本质上就是调参的过程，不管是分类还是回归都是找出每个神经元最优的w，b，h（模型确定的情况下），使x不变的情况下y最接近真实值\n",
    "\n",
    "## 激活函数\n",
    "\n",
    "h(x)可以使线性也可以是非线性\n",
    "h(x) = cx\n",
    "y(x) = h(h(h(x))) 就是一个3层神经网络\n",
    "相当于 h(x) = c ** 3 * x ，那其实本质上还是一层，那效果就不好了，因为隐藏层数应该越深越好（也不尽然，resnet就是为了防止过拟合）\n",
    "\n",
    "### Sigmoid 激活函数\n",
    "\n",
    "$函数公式在此$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e41c735",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "604465c8",
   "metadata": {},
   "source": [
    "图像在此\n",
    "\n",
    "- 优点：简单，适合做分类任务（在输出层的激活函数来判断属于哪个分类，将数转为概率）\n",
    "- 缺点：\n",
    "1. 反向传播训练时候有梯度消失的问题 (w,b就不更新了)\n",
    "2. 输出值区间（0,1） 关于0不对称\n",
    "3. 梯度更新在不同方向走得太远，使得优化难度增大，训练耗时"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bf3994e",
   "metadata": {},
   "source": [
    "### Tanh 激活函数 （双曲正切）\n",
    "\n",
    "$函数公式在此$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36c4874c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8b08b5ba",
   "metadata": {},
   "source": [
    "图像在此\n",
    "\n",
    "- 优点：\n",
    "1. 解决了Sigmoid函数输出值非0对称的问题\n",
    "2. 训练速度快，更容易收敛\n",
    "- 缺点：\n",
    "1. 梯度消失的问题\n",
    "2. 与Sigmoid函数类似"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac43617f",
   "metadata": {},
   "source": [
    "### ReLU函数 \n",
    "\n",
    "$公式$ 本质上是分段函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9d19a922",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 图像"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02536eac",
   "metadata": {},
   "source": [
    "- 优点：\n",
    "1. 解决梯度消失的问题\n",
    "2. 计算更为简单，没有T和S的指数运算\n",
    "- 缺点：\n",
    "会有神经元死亡的问题"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06d4bc0b",
   "metadata": {},
   "source": [
    "### Leaky ReLU函数\n",
    "\n",
    "$公式$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "708678a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 图像"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57d976ed",
   "metadata": {},
   "source": [
    "- 优点：\n",
    "解决神经元死亡问题\n",
    "- 缺点：\n",
    "无法为正负输入值提供一致的关系预测（不同区间函数不同）\n",
    "\n",
    "### Softmax函数"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75f31ada",
   "metadata": {},
   "source": [
    "## 前向传播\n",
    "\n",
    "前向传播其实就是从输入层开始，正向的进行函数求果\n",
    "这里w，b是需要一个初始值的\n",
    "\n",
    "不管在训练，推理，验证，测试还是传播的过程中都需要正向传播\n",
    "\n",
    "y的label就是一个y的真实值，<mark>损失函数</mark>就是用来表示y的真实值和y的预测值之间的差异的\n",
    "\n",
    "## 损失函数\n",
    "\n",
    "### 均方误差的损失函数\n",
    "\n",
    "$公式$\n",
    "\n",
    "来判断预测效果\n",
    "\n",
    "### 交叉熵损失函数\n",
    "\n",
    "用于图像分类\n",
    "\n",
    "神经网络就是在不断进行着这样的过程：前向传播，计算误差，反向传播更新w和b，循环往复直到误差收敛\n",
    "\n",
    "<mark>梯度下降就是反向传播的具体过程</mark>\n",
    "\n",
    "\n",
    "## 梯度下降法\n",
    "\n",
    "$w和b的参数更新计算公式$\n",
    "\n",
    "a是超参数，也就是学习率，0.05到0.001之间"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
