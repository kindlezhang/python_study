{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "beb684c0",
   "metadata": {},
   "source": [
    "# basic knowledge\n",
    "\n",
    "## DNN结构\n",
    "\n",
    "输入层 隐藏层 输出层\n",
    "输入层是一些图片或者矩阵\n",
    "同层的神经单元之间相互独立，不同层之间的神经元全连接\n",
    "\n",
    "## DNN单元\n",
    "\n",
    "神经元的数学表达式：\n",
    "a = h(w * x + b)\n",
    "b是bias, w是权重矩阵，x是输入矩阵，a是下一个神经元，h是<mark>激活函数</mark>\n",
    "本质上就是调参的过程，不管是分类还是回归都是找出每个神经元最优的w，b，h（模型确定的情况下），使x不变的情况下y最接近真实值\n",
    "\n",
    "## 激活函数\n",
    "\n",
    "h(x)可以使线性也可以是非线性\n",
    "h(x) = cx\n",
    "y(x) = h(h(h(x))) 就是一个3层神经网络\n",
    "相当于 h(x) = c ** 3 * x ，那其实本质上还是一层，那效果就不好了，因为隐藏层数应该越深越好（也不尽然，resnet就是为了防止过拟合）\n",
    "\n",
    "### Sigmoid 激活函数\n",
    "\n",
    "$函数公式在此$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e41c735",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "604465c8",
   "metadata": {},
   "source": [
    "图像在此\n",
    "\n",
    "- 优点：简单，适合做分类任务（在输出层的激活函数来判断属于哪个分类，将数转为概率）\n",
    "- 缺点：\n",
    "1. 反向传播训练时候有梯度消失的问题 (w,b就不更新了)\n",
    "2. 输出值区间（0,1） 关于0不对称\n",
    "3. 梯度更新在不同方向走得太远，使得优化难度增大，训练耗时"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bf3994e",
   "metadata": {},
   "source": [
    "### Tanh 激活函数 （双曲正切）\n",
    "\n",
    "$函数公式在此$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36c4874c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8b08b5ba",
   "metadata": {},
   "source": [
    "图像在此\n",
    "\n",
    "- 优点：\n",
    "1. 解决了Sigmoid函数输出值非0对称的问题\n",
    "2. 训练速度快，更容易收敛\n",
    "- 缺点：\n",
    "1. 梯度消失的问题\n",
    "2. 与Sigmoid函数类似"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac43617f",
   "metadata": {},
   "source": [
    "### ReLU函数 \n",
    "\n",
    "$公式$ 本质上是分段函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9d19a922",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 图像"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02536eac",
   "metadata": {},
   "source": [
    "- 优点：\n",
    "1. 解决梯度消失的问题\n",
    "2. 计算更为简单，没有T和S的指数运算\n",
    "- 缺点：\n",
    "会有神经元死亡的问题"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06d4bc0b",
   "metadata": {},
   "source": [
    "### Leaky ReLU函数\n",
    "\n",
    "$公式$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "708678a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 图像"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57d976ed",
   "metadata": {},
   "source": [
    "- 优点：\n",
    "解决神经元死亡问题\n",
    "- 缺点：\n",
    "无法为正负输入值提供一致的关系预测（不同区间函数不同）\n",
    "\n",
    "### Softmax函数"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75f31ada",
   "metadata": {},
   "source": [
    "## 前向传播\n",
    "\n",
    "前向传播其实就是从输入层开始，正向的进行函数求果\n",
    "这里w，b是需要一个初始值的\n",
    "\n",
    "不管在训练，推理，验证，测试还是传播的过程中都需要正向传播\n",
    "\n",
    "y的label就是一个y的真实值，<mark>损失函数</mark>就是用来表示y的真实值和y的预测值之间的差异的\n",
    "\n",
    "## 损失函数\n",
    "\n",
    "### 均方误差的损失函数\n",
    "\n",
    "$公式$\n",
    "\n",
    "来判断预测效果\n",
    "\n",
    "### 交叉熵损失函数\n",
    "\n",
    "用于图像分类\n",
    "\n",
    "神经网络就是在不断进行着这样的过程：前向传播，计算误差，反向传播更新w和b，循环往复直到误差收敛\n",
    "\n",
    "<mark>梯度下降就是反向传播的具体过程</mark>\n",
    "\n",
    "\n",
    "## 梯度下降法\n",
    "\n",
    "$w和b的参数更新计算公式$\n",
    "\n",
    "a是超参数，也就是学习率，0.05到0.001之间\n",
    "\n",
    "后续梯度下降算法会有优化，比如adam等，但是基础都是这样\n",
    "\n",
    "### 计算案例\n",
    "\n",
    "<!-- ![计算过程](../graph/01.png \"过程1\") -->\n",
    "<img src=\"../graph/01.png\" width=\"600\" height=\"400\" />\n",
    "<img src=\"../graph/02.png\" width=\"600\" height=\"400\" />\n",
    "<img src=\"../graph/03.png\" width=\"600\" height=\"400\" />\n",
    "\n",
    "设置一个学习率，从后向前更新系数，动手算一下加深理解。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d8e40c87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.0528886100419586"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "1/(1+math.exp(-1))\n",
    "(math.exp(1)/(1+math.exp(1))**2)*(1.731-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "286a3e53",
   "metadata": {},
   "source": [
    "## 深度学习过程\n",
    "\n",
    "- 建立model\n",
    "\n",
    "- train and validate 产生权重，包含了模型的结构和训练好的参数w,b\n",
    "  1. 数据处理\n",
    "  2. 模型训练\n",
    "  3. loss ACC \n",
    "\n",
    "- test\n",
    "    输入模型，根据label进行评估\n",
    "\n",
    "## 卷积神经网络\n",
    "\n",
    "CNN应用在CV领域有很多比如图像分类ResNet，图像分割有UNet, 目标检测有Yolo\n",
    "\n",
    "输入可以是图像，视频\n",
    "\n",
    "图像的本质就是数字矩阵特征图，数字代表像素颜色，255是白色，0是黑色（专指单通道灰度图，彩色图是red，green和blue的三通道图）\n",
    "\n",
    "输入的数据往往呈这样的格式：\n",
    "\n",
    "128 * 28 * 28 * 1\n",
    "\n",
    "就是batch * 大小 * 通道\n",
    "\n",
    "### DNN相比CNNC在图像分割上的缺陷\n",
    "\n",
    "假设一个28*28*1的图像，放入DNN，就要把数字矩阵变成一个一维数组，长度是28*28，那么就会丢失有效信息，图像特征会缺失\n",
    "CNN就避免了这个问题，他把2维数据压缩成另一个2维数据，比如6*6\n",
    "\n",
    "### 卷积运算\n",
    "\n",
    "比如： <img src=\"../graph/04.png\" width=\"300\" height=\"200\" />\n",
    "\n",
    "卷积核其实就是DNN中的w构成了矩阵，这个核大小不变，w大小需要初始值，卷积的过程就是提取特征的过程\n",
    "\n",
    "y = model(CNN)(x), output = 激活函数f(y)\n",
    "\n",
    "偏置也可以加入进去 y = model(CNN)(x) + b\n",
    "\n",
    "卷积核大小需要设定，更新依然使用梯度下降法则，激活函数使用图像分类专用的，损失函数也是专用的比如<mark>交叉熵</mark>\n",
    "\n",
    "### 步幅\n",
    "\n",
    "步幅代表卷积核在进行卷积操作时，每次滑动的距离\n",
    "\n",
    "步幅过大会在外层添加0\n",
    "\n",
    "### 填充\n",
    "\n",
    "步幅和填充操作可以共同控制卷积后的特征图的大小，如果不填充只卷积，那么特征图会越来越小，有时候就不支持继续卷积了，所以要填充\n",
    "\n",
    "### 特征图大小公式\n",
    "\n",
    "<img src=\"../graph/05.png\" width=\"500\" height=\"250\" />\n",
    "\n",
    "H是输入特征图高\n",
    "W是输入特征图宽\n",
    "F是卷积核\n",
    "P是填充\n",
    "S是步幅\n",
    "\n",
    "一般如果是小数，那么框架tensorflow或者pytorch会向下取整\n",
    "\n",
    "### 多通道卷积运算\n",
    "\n",
    "类似的，输入特征图有几个通道，卷积核就有几个通道，通道之间相互独立地进行卷积，<mark>最后把所有的输出特征图对应位置相加得到最终的特征图</mark>\n",
    "\n",
    "如果有多个卷积核的话，最后输出特征图经过激活函数后就会有多个通道\n",
    "\n",
    "<img src=\"../graph/06.png\" width=\"500\" height=\"250\" />\n",
    "\n",
    "如果有偏置的话就是类似这样\n",
    "\n",
    "<img src=\"../graph/07.png\" width=\"500\" height=\"250\" />\n",
    "\n",
    "### 池化运算\n",
    "\n",
    "最大池化：找某个区域中的最大值，最为代表这个区域的特征值\n",
    "平均池化：类似找平均值\n",
    "\n",
    "也有S和P，但是FH和FW变成了感受野，因为没有池化核\n",
    "\n",
    "<img src=\"../graph/08.png\" width=\"500\" height=\"250\" />\n",
    "\n",
    "<mark>由于没有核，所以输入多少通道，输出就多少通道，不用像卷积运算那样加起来</mark>\n",
    "\n",
    "### CNN整体结构\n",
    "\n",
    "如图是一个单通道，卷积核数量为3的CNN架构\n",
    "\n",
    "<img src=\"../graph/09.png\" width=\"500\" height=\"250\" />\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
