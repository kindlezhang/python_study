{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "18930950",
   "metadata": {},
   "source": [
    "# AlexNet\n",
    "\n",
    "LeNet在小数据集上效果较好，但是在大数据集上表现不好，2012年前经常被machine learning超越，比如SVM\n",
    "\n",
    "<img src=\"../graph/14.png\" width=\"1200\" height=\"250\" />\n",
    "\n",
    "### 参数计算\n",
    "\n",
    "<img src=\"../graph/15.png\" width=\"500\" height=\"250\" />\n",
    "\n",
    "<mark>除了卷积使用了ReLU,最后三个DNN，前两层也使用了ReLU并又使用了Dropout，最后一层使用的softmax\n",
    "\n",
    "同时使用的是最大池化而不是平均池化\n",
    "\n",
    "特征提取后有6*6*256个数，展平后，全连接到DNN中，参数有一些太多了,容易导致过拟合（训练效果好，实验效果差），所以要使用dropout使一些神经元随机失活\n",
    "\n",
    "### Dropout\n",
    "\n",
    "每一轮都会随机失活一部分神经元，对应的weight在本轮就不会更新，既可以加速训练过程也可以防止过拟合\n",
    "\n",
    "<img src=\"../graph/16.png\" width=\"500\" height=\"250\" />\n",
    "\n",
    "主要在AlexNet中使用\n",
    "\n",
    "### 图像增强-水平翻转\n",
    "\n",
    "在项目中两个难点：\n",
    "1. 查找数据，最耗时\n",
    "2. 数据扩增：可以让数据足够多，同时也可以防止数据过拟合（比如图片翻转或者裁剪后再训练一次，使网络更加健壮\n",
    "\n",
    "### 图像增强-PCA\n",
    "\n",
    "<img src=\"../graph/17.png\" width=\"500\" height=\"250\" />\n",
    "\n",
    "### LRN正则化\n",
    "\n",
    "<img src=\"../graph/18.png\" width=\"500\" height=\"250\" />\n",
    "\n",
    "<img src=\"../graph/19.png\" width=\"500\" height=\"250\" />\n",
    "\n",
    "LRN以通道为单位，对通道中的每个数进行计算，\n",
    "k是为了防止分母为0，n是范围，n=2就是左右各1个的范围，n=4就是左右两个。\n",
    "N就是总的通道数。\n",
    "\n",
    "### AlexNet总结\n",
    "\n",
    "1. 卷积核很大，参数更多\n",
    "2. 使用了ReLU代替sigmoid或者Tanh，更快收敛，耗时少\n",
    "3. Dropout减少过拟合\n",
    "4. 使用重叠的最大池化，提出步长小于感受野，池化层之间就会有重叠，提升特征的丰富性\n",
    "5. 使用两种数据扩增技术，大幅增加了训练数据，增加鲁棒性\n",
    "6. 使用LRN，增加对比度，多GPU进行训练（后面不再采用）\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
